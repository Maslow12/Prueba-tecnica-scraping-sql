# Prueba TÃ©cnica: Scraping Web + Almacenamiento SQL (Perfil Mid-Level)
* By Sebastian Yusti

ðŸ“‹ Requisitos Previos
Python 3.13 instalado (Descargar Python: https://www.python.org/downloads/release/python-3133/)

ðŸ”§ Pasos para Instalar el Script:


1ï¸âƒ£ Clonar el Repositorio (Opcional)


```
git clone https://github.com/Maslow12/Prueba-tecnica-scraping-sql.git
```

2ï¸âƒ£ Crear un Entorno Virtual (Virtual Environment)
ðŸ“Œ Recomendado para evitar conflictos entre dependencias.

```
python -m venv venv
.\venv\Scripts\activate
```

3ï¸âƒ£ Instalar Dependencias (requirements.txt)
Si el proyecto incluye un archivo requirements.txt:

```
pip install -r requirements.txt
```

4ï¸âƒ£ Ejecutar el Script

```
python main.py
```

### Requirements:

```
beautifulsoup4==4.13.4
certifi==2025.4.26
charset-normalizer==3.4.2
free_proxy==1.1.3
idna==3.10
lxml==5.4.0
numpy==2.2.5
pandas==2.2.3
python-dateutil==2.9.0.post0
pytz==2025.2
requests==2.32.3
six==1.17.0
soupsieve==2.7
typing_extensions==4.13.2
tzdata==2025.2
urllib3==2.4.0
```

* __BeautifulSoup:__ Se optÃ³ por utilizar BeautifulSoup (BS4) para el scraping debido a la prueba no requiere un alcance complejo. No obstante, para proyectos mÃ¡s complejos o de mayor escala, Scrapy serÃ­a la opciÃ³n recomendada, ya que ofrece una infraestructura robusta con funcionalidades avanzadas como manejo de peticiones concurrentes, pipelines integrados y soporte para crawling automatizado.
  
* __FreeProxy:__ Esta libreria no es recomendada para entornos productivos, ya que es mejor utilizar opciones pagas para obtener proxies. Su funciÃ³n principal es extraer proxies activos de sitios como sslproxies.org, us-proxy.org y free-proxy-list.net, verificando su disponibilidad para implementar un sistema de proxies rotativos. Dado que IMDB emplea Cloudflare para bloquear bots, y al no poder usar herramientas como Selenium o Playwright en este ejercicio, se optÃ³ por requests con proxies para evitar el bloqueo y extraer el HTML. Sin embargo, esta soluciÃ³n presenta limitaciones de rendimiento, las cuales se optimizaron mediante programaciÃ³n concurrente, combinando ThreadPoolExecutor para I/O y multiprocessing.Pool.

* __Pandas:__ Es la libreria que facilita exportar los datos desde un ```dict``` a un archivo .xslx

* __Sqlite3:__: Base de datos utilizada


### Notas:

* Un desafÃ­o tÃ©cnico clave fue la imposibilidad de utilizar frameworks de automatizaciÃ³n de navegadores (como Selenium o Playwright) para manejar la carga dinÃ¡mica de contenido mediante scroll en JavaScript. Esto limitaba la visualizaciÃ³n inicial a solo las primeras 25 pelÃ­culas, cuando el objetivo requerÃ­a obtener al menos 50. Como soluciÃ³n alternativa, se implementÃ³ una estrategia basada en los parÃ¡metros de la URL: se extrajeron los primeros 25 resultados ordenados ascendentemente (```?sort=rank%2Casc```) y los Ãºltimos 25 ordenados descendentemente (```?sort=rank%2Cdesc```), combinando ambos conjuntos para alcanzar el volumen de datos deseado.
  
* Para estandarizar el formato de duraciÃ³n de las pelÃ­culas (que originalmente aparecÃ­a en horas y minutos), se implementÃ³ la funciÃ³n ```time_to_minutes(time_str:str)->int```, la cual convierte este dato a una representaciÃ³n numÃ©rica en minutos.

* Como se mencionÃ³ previamente, para optimizar el rendimiento del script se implementÃ³ programaciÃ³n concurrente.EspecÃ­ficamente, ThreadPoolExecutor se empleÃ³ para extraer de manera eficiente la informaciÃ³n individual de cada pelÃ­cula mediante mÃºltiples solicitudes web simultÃ¡neas, mientras que multiprocessing.Pool permitiÃ³ ejecutar en paralelo ambos filtros (ascendente ```?sort=rank%2Cdesc```y descendente ```?sort=rank%2Casc```) para obtener los datos de las pÃ¡ginas de forma concurrente, maximizando asÃ­ la eficiencia del proceso de scraping.

# Preguntas adicionales:

1. CÃ³mo configurarÃ­as Selenium o Playwright para acceder a las pÃ¡ginas de
IMDB (en tÃ©rminos de configuraciÃ³n de navegador, manejo de headers,
cookies, etc.):

âœ… Utilizar Undetected-Chromedriver (evita la detecciÃ³n de Selenium).
âœ… ```WebDriverWait(driver, time:int)``` Para simular el comportamiento humano
âœ… Anadir argumentos como ```
    options.add_argument("--disable-blink-features=AutomationControlled") 
    options.add_argument("--headless=new")  # Optional: Run in headless mode```


2. CÃ³mo navegarÃ­as entre las pÃ¡ginas de IMDB para obtener el listado de pelÃ­culas y acceder a la pÃ¡gina de detalle de cada una.

âœ… Para obtener todas las peliculas es necesario realizar un "scroll down" para ello se ejecuta una secuencia de Javascript para realizarlo con ```driver.execute_script("window.scrollTo(0, document.body.scrollHeight)")```   

âœ… Es necesario esperar que todo el DOM cargue se puede hacer con un ```WebDriverWait(driver, 5).until(
            lambda d: d.execute_script("return document.body.scrollHeight") > last_height
        )```

3. CÃ³mo extraerÃ­as los datos de cada pÃ¡gina (tÃ­tulo, aÃ±o, calificaciÃ³n,
duraciÃ³n, metascore, elenco) utilizando estas tecnologÃ­as.

âœ… Con el WebDriver se obtiene el ```driver.page_source``` y se pueden utilizar las mismas funciones hechas en este ejercicio para obtener la data con ```BeautifulSoup(driver.page_source, 'html.parser')```

![alt text](image-1.png) __Imagen 1__

âœ… Con el estilo de vista mostrado en la imagen se puede obtener bastante informaciÃ³n sin necesidad de entrar a cada pÃ¡gina del detalle, esto mejora considerablemente la eficiencia del script, se puede realizar con un:
```
button = driver.find_element(By.ID, "list-view-option-detailed")
driver.execute_script("arguments[0].click();", button)
```

âœ… Al igual que en el ejercicio anterior, donde se implementÃ³ un sistema para acceder a cada pÃ¡gina de detalles mediante los enlaces href utilizando programaciÃ³n concurrente, es posible aplicar la misma configuraciÃ³n del driver de Selenium para navegar eficientemente a las pÃ¡ginas individuales de cada pelÃ­cula.

4. CÃ³mo manejarÃ­as el tiempo de espera entre solicitudes (esperas explÃ­citas
o implÃ­citas) para evitar bloqueos y errores.

âœ… Para manejar correctamente el scroll y contenido dinÃ¡mico, se recomienda implementar "Explicit Waits" (esperas explÃ­citas) que verifiquen activamente la carga del contenido o la finalizaciÃ³n del scroll automÃ¡tico. Se puede usar lo siguente ```WebDriverWait(driver, 15).until(
    lambda d: d.execute_script("return (window.scrollY + window.innerHeight) >= document.body.scrollHeight")
)```

5. CÃ³mo manejarÃ­as los posibles problemas que puedan surgir al utilizar
estas tecnologÃ­as (por ejemplo, bloquear IP, captchas, JavaScript cargado
dinÃ¡micamente).

âœ… Para manejar los bloqueos de IP, Proxies rotativos (SmartProxy) o Plataformas pagas proveedoras de proxies, librerias como ```random_user_agent``` para obtener distintos user agents

âœ… Usar presolved Captcha, o librerias como TwoCaptcha (Paga)

âœ… Utilizar Explicit Waits


# IMPORTANTE

* En la carpeta ```results``` estan los excel con los datos obtenidos de la DB ya cargada
* El nombre de la db es result.db esta en SQLite3
